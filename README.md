# kaggle_titanic

Using ML tools learned recently in courses on the Kaggle Titanic challenge:
===========================================================================

First, tools learned in the Andrew Ng Coursera Machine Learning course offered by Stanford.   (Octave Gnu)
https://www.coursera.org/learn/machine-learning/home/welcome

I used two algorithms:
* Support Vector Machine - score: 0.78947
* Neural Network
- 7x7 lambda 0.005: score:  
- 7x2 lambda 0.3: score: 


Second, tools from Machine Learning A-Zª: Hands-On Python & R In Data Science
(Python, Jupyter Notebooks)
https://www.udemy.com/course/machinelearning/


* Kernel Support Vector Machine - score: linear: 0.76555, rbf: 0.78229, poly: 0.76794, sigmoid: 0.63875
* Decision tree - score: entropy: 0.72488, gini: 0.70574 
* Logistic Regression - score: 0.76315
* Naive Bayes - score: 0.74401
* Random Forest - score: 0.76076
* CatBoost - score: 0.76794
* Neural nets:
- 7x2 Adam score:
- 7x2 rmsprop score: 
- 7x4 Adam score:
- 7x4 nadam score: 

