# Using ML tools learned recently in courses on the Kaggle Titanic challenge


---


## First
Tools learned in the Andrew Ng [Coursera Machine Learning course](https://www.coursera.org/learn/machine-learning/home/welcome) offered by Stanford.   (Octave Gnu)



* Support Vector Machine - score: 0.78947
* Logistic Regression - score: 0.76315
* Neural Network
  - 7x7 lambda 0.005: score: 0.75358  
  - 7x2 lambda 0.3: score: 0.77990



## Second
Tools from [Machine Learning A-ZÂª: Hands-On Python & R In Data Science](https://www.udemy.com/course/machinelearning/)
(Python, Jupyter Notebooks)



* Kernel Support Vector Machine - scores:
	* linear: 0.76555
	* rbf: 0.78229
	* poly: 0.76794
	* sigmoid: 0.63875
* Decision tree - scores: 
	* entropy: 0.72488
	* gini: 0.70574 

* Naive Bayes - score: 0.74401
* Random Forest - score: 0.76076
* CatBoost - score: 0.76794
* Neural nets:
  - 7x2 adam score: 0.76794
  - 7x2 rmsprop score: 0.77272
  - 7x4 adam score: 0.75837
  - 7x4 nadam score: 0.77511

